{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devt17/wadassign2a/blob/master/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn nltk"
      ],
      "metadata": {
        "id": "xkWw5dvOc-sb",
        "outputId": "3dd6e55d-ac2c-4ec8-ee0b-0c435afdcea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "m0M9OaohdRce",
        "outputId": "ab6e464e-368a-4682-bcc5-a5c590960b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "g7buRWVNdZPo",
        "outputId": "587b4b98-f7d8-4a58-b32e-20420497607c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Uncomment the following line to download NLTK resources (run once)\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# Sample documents (replace with your data)\n",
        "documents = [\n",
        "    \"Information retrieval is the process of obtaining information from a large repository.\",\n",
        "    \"Research papers on information retrieval cover various algorithms and methods.\",\n",
        "    \"Machine learning techniques are being applied to improve information retrieval systems.\",\n",
        "    \"The vector space model is one of the key models in information retrieval.\"\n",
        "]\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(doc):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    return ' '.join([word for word in tokens if word.isalnum() and word not in stop_words])\n",
        "\n",
        "# Preprocess documents\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# 1. Indexing with TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
        "\n",
        "# 2. Querying using Vector Space Model\n",
        "def query(query_string):\n",
        "    query_processed = preprocess(query_string)\n",
        "    query_vec = vectorizer.transform([query_processed])\n",
        "    return query_vec\n",
        "\n",
        "# 3. Cosine Similarity\n",
        "def retrieve_documents(query_vec):\n",
        "    cosine_sim = cosine_similarity(query_vec, tfidf_matrix)\n",
        "    return cosine_sim.flatten()\n",
        "\n",
        "# 4. Ranking with BM25\n",
        "def bm25_score(query, documents, k1=1.5, b=0.75):\n",
        "    # Implementing BM25 scoring\n",
        "    avg_doc_len = sum(len(doc.split()) for doc in documents) / len(documents)\n",
        "    scores = []\n",
        "\n",
        "    for doc in documents:\n",
        "        score = 0\n",
        "        doc_len = len(doc.split())\n",
        "        for word in query.split():\n",
        "            f = doc.lower().split().count(word.lower())\n",
        "            if f > 0:\n",
        "                score += ((f * (k1 + 1)) / (f + k1 * (1 - b + b * (doc_len / avg_doc_len))))\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# 5. Clustering Similar Documents\n",
        "def cluster_documents(n_clusters=2):\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    cluster_labels = clustering.fit_predict(tfidf_matrix.toarray())\n",
        "    return cluster_labels\n",
        "\n",
        "# Example usage\n",
        "query_string = \"information retrieval algorithms\"\n",
        "query_vec = query(query_string)\n",
        "\n",
        "# Retrieve and rank documents\n",
        "similarity_scores = retrieve_documents(query_vec)\n",
        "bm25_scores = bm25_score(query_string, documents)\n",
        "\n",
        "# Display results\n",
        "print(\"Cosine Similarity Scores:\", similarity_scores)\n",
        "print(\"BM25 Scores:\", bm25_scores)\n",
        "\n",
        "# Clustering documents\n",
        "clusters = cluster_documents()\n",
        "print(\"Document Clusters:\", clusters)\n"
      ],
      "metadata": {
        "id": "yNhOom0jc_Ff",
        "outputId": "196ff702-b2eb-4e5e-f8e0-0ab049b3e131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Scores: [0.2838823  0.48581447 0.17129756 0.17129756]\n",
            "BM25 Scores: [2.3896923172368307, 3.1870669745958433, 2.0399113082039912, 0.944558521560575]\n",
            "Document Clusters: [0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(retrieved_docs, relevant_docs):\n",
        "    correct_predictions = len(set(retrieved_docs) & set(relevant_docs))  # Intersection\n",
        "    total_retrieved = len(retrieved_docs)\n",
        "\n",
        "    if total_retrieved == 0:  # Avoid division by zero\n",
        "        return 0.0\n",
        "\n",
        "    accuracy = (correct_predictions / total_retrieved) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Example usage\n",
        "retrieved_docs = [\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\", \"doc6\", \"doc7\", \"doc8\", \"doc9\", \"doc10\"]\n",
        "relevant_docs = [\"doc1\", \"doc3\", \"doc5\", \"doc7\", \"doc9\"]\n",
        "\n",
        "accuracy = calculate_accuracy(retrieved_docs, relevant_docs)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "iRykCB74dHVM",
        "outputId": "d639df3a-246e-497d-da23-4c4561c14e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Uncomment the following lines to download NLTK resources (run once)\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# Sample documents (replace with your own data)\n",
        "documents = [\n",
        "    \"Information retrieval is the process of obtaining information from a large repository.\",\n",
        "    \"Research papers on information retrieval cover various algorithms and methods.\",\n",
        "    \"Machine learning techniques are being applied to improve information retrieval systems.\",\n",
        "    \"The vector space model is one of the key models in information retrieval.\",\n",
        "    \"This paper discusses the evaluation of information retrieval systems.\",\n",
        "    \"Recent advancements in machine learning have influenced retrieval techniques.\",\n",
        "]\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(doc):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    return ' '.join([word for word in tokens if word.isalnum() and word not in stop_words])\n",
        "\n",
        "# Preprocess documents\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# 1. Indexing with TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
        "\n",
        "# 2. Querying using Vector Space Model\n",
        "def query(query_string):\n",
        "    query_processed = preprocess(query_string)\n",
        "    query_vec = vectorizer.transform([query_processed])\n",
        "    return query_vec\n",
        "\n",
        "# 3. Cosine Similarity\n",
        "def retrieve_documents(query_vec):\n",
        "    cosine_sim = cosine_similarity(query_vec, tfidf_matrix)\n",
        "    return cosine_sim.flatten()\n",
        "\n",
        "# 4. BM25 Implementation\n",
        "def bm25_score(query, documents, k1=1.5, b=0.75):\n",
        "    avg_doc_len = sum(len(doc.split()) for doc in documents) / len(documents)\n",
        "    scores = []\n",
        "\n",
        "    for doc in documents:\n",
        "        score = 0\n",
        "        doc_len = len(doc.split())\n",
        "        for word in query.split():\n",
        "            f = doc.lower().split().count(word.lower())\n",
        "            if f > 0:\n",
        "                score += ((f * (k1 + 1)) / (f + k1 * (1 - b + b * (doc_len / avg_doc_len))))\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# 5. Clustering Similar Documents\n",
        "def cluster_documents(n_clusters=2):\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    cluster_labels = clustering.fit_predict(tfidf_matrix.toarray())\n",
        "    return cluster_labels\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(retrieved_docs, relevant_docs):\n",
        "    correct_predictions = len(set(retrieved_docs) & set(relevant_docs))  # Intersection\n",
        "    total_retrieved = len(retrieved_docs)\n",
        "\n",
        "    if total_retrieved == 0:  # Avoid division by zero\n",
        "        return 0.0\n",
        "\n",
        "    accuracy = (correct_predictions / total_retrieved) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Function to calculate precision\n",
        "def calculate_precision(retrieved_docs, relevant_docs):\n",
        "    true_positives = len(set(retrieved_docs) & set(relevant_docs))\n",
        "    false_positives = len(set(retrieved_docs) - set(relevant_docs))\n",
        "\n",
        "    if true_positives + false_positives == 0:  # Avoid division by zero\n",
        "        return 0.0\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "    return precision * 100\n",
        "\n",
        "# Function to calculate recall\n",
        "def calculate_recall(retrieved_docs, relevant_docs):\n",
        "    true_positives = len(set(retrieved_docs) & set(relevant_docs))\n",
        "    false_negatives = len(set(relevant_docs) - set(retrieved_docs))\n",
        "\n",
        "    if true_positives + false_negatives == 0:  # Avoid division by zero\n",
        "        return 0.0\n",
        "\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "    return recall * 100\n",
        "\n",
        "# Example usage\n",
        "query_string = \"information retrieval algorithms\"\n",
        "query_vec = query(query_string)\n",
        "\n",
        "# Retrieve and rank documents using cosine similarity\n",
        "similarity_scores = retrieve_documents(query_vec)\n",
        "print(\"Cosine Similarity Scores:\", similarity_scores)\n",
        "\n",
        "# Ranking with BM25\n",
        "bm25_scores = bm25_score(query_string, documents)\n",
        "print(\"BM25 Scores:\", bm25_scores)\n",
        "\n",
        "# Clustering documents\n",
        "n_clusters = 2  # You can change this number\n",
        "clusters = cluster_documents(n_clusters)\n",
        "print(\"Document Clusters:\", clusters)\n",
        "\n",
        "# Example for evaluation\n",
        "# Assume these are the retrieved documents based on your algorithm\n",
        "retrieved_docs = [documents[i] for i in np.argsort(-similarity_scores)[:3]]  # Top 3 based on cosine similarity\n",
        "relevant_docs = [\n",
        "    \"Information retrieval is the process of obtaining information from a large repository.\",\n",
        "    \"Research papers on information retrieval cover various algorithms and methods.\"\n",
        "]\n",
        "\n",
        "# Calculate accuracy, precision, and recall\n",
        "accuracy = calculate_accuracy(retrieved_docs, relevant_docs)\n",
        "precision = calculate_precision(retrieved_docs, relevant_docs)\n",
        "recall = calculate_recall(retrieved_docs, relevant_docs)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n"
      ],
      "metadata": {
        "id": "cUzRPFNOdkaC",
        "outputId": "9760a3fb-eab9-4c92-ec59-014182db2e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Scores: [0.26090346 0.47534126 0.16762274 0.14965916 0.18712256 0.07142734]\n",
            "BM25 Scores: [2.3201361200700914, 3.0868167202572345, 1.9722650231124808, 0.9103840682788051, 2.1512605042016806, 1.0756302521008403]\n",
            "Document Clusters: [0 0 1 0 0 1]\n",
            "Accuracy: 66.67%\n",
            "Precision: 66.67%\n",
            "Recall: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jjVjSKodyyq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}